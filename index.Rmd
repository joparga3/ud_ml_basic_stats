---
title: "Basic Stats"
author: "Jose Parreno Garcia"
date: "February 2018"
output: 
  html_document:
    toc: true # table of content true
    depth: 6  # upto three depths of headings (specified by #, ##, ###, ####)
    number_sections: true  ## if you want number sections at each table header
    #theme: spacelab  # many options for theme, this one is my favorite.
    #highlight: tango  # specifies the syntax highlighting style
    keep_md: true
---
<style>
body {
text-align: justify}

</style>

<br>

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 250)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source_path = getwd()
```

```{r results='hide', message=FALSE, warning=FALSE}
library(knitr)
```

<br>

We will look at:

* Understanding data sampling in R
* Operating a probability distribution in R
* Working with univariate descriptive statistics in R
* Performing correlations and multivariate analysis
* Operating linear regression and multivariate analysis
* Conducting an statistical tests

# Understanding data sampling in R

There are several reasons why sampling are used:

* When using machine learning or predictive modelling, we can sample our data to create different datasets which we use to train/test/validate
* When you have a massive dataset and little computational power, you want to extract samples of the data that are representative of the wider population to model on those.

```{r fig.width=7, fig.height=7}
# Randomnly sample a vector
sample(1:10)

# Randomnly sample a vector and return X number of elements
sample(1:10, size = 5)

# Randomnly sample a vector with Bernouilli trials - ie - being able to repeat elements.
sample(c(0,1), 10, replace = TRUE)

# Randomnly sample a vector that can only return integers
sample.int(20, 12)
```

<br>

# Operating a probability distribution in R

Remember the second point above?

* When you have a massive dataset and little computational power, you want to extract samples of the data that are representative of the wider population to model on those.

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/1.PNG"))
```

Creating different distributions:

```{r fig.width=12, fig.height=7}
library(stats)

# Normal distribution -> returning "height" of the curve at point 0
dnorm(0)

# Idem as above, but having a mean and standard deviation
dnorm(0,mean=3,sd=5)

# Plotting a normal distribution
curve(dnorm,-3,3)

# Normal distribution -> returning the area under/left a given value
pnorm(1.5)

# Normal distribution -> returning the area above/right a given value
pnorm(1.5, lower.tail=FALSE)

# Plotting
curve(pnorm(x), -3,3)

# Normal distribution -> quantiles
qnorm(0.5)

# Generate random numbers from a normal distribution
set.seed(50)
x = rnorm(100,mean=3,sd=5)
hist(x)

# Generate random numbers from a uniform distribution
set.seed(50)
y = runif(100,0,5)
hist(y)
```

## Testing normality of the data with a shapiro test

```{r fig.width=12, fig.height=7}
# p-value is above 0.05 -> normal distribution
shapiro.test(x)

# p-value is below 0.05 -> not a normal distribution
shapiro.test(y)
```

# Working with univariate descriptive statistics in R

We say univariate descriptive statistics because we are generating calculating for a single variable.

## Basic stats

```{r fig.width=12, fig.height=7}
data(mtcars)

range(mtcars$mpg)

length(mtcars$mpg)

mean(mtcars$mpg)

median(mtcars$mpg)

sd(mtcars$mpg)

var(mtcars$mpg)

sd(mtcars$mpg) ^ 2

IQR(mtcars$mpg)

quantile(mtcars$mpg,0.67)

max(mtcars$mpg)

min(mtcars$mpg)

cummax(mtcars$mpg)

cummin(mtcars$mpg)

summary(mtcars)

table(mtcars$cyl)

stem(mtcars$mpg)
```

## Basic histogram and mode

```{r fig.width=12, fig.height=7}
library(ggplot2)

qplot(mtcars$mpg, binwidth=2)

mode = function(x) {
   temp = table(x)
   names(temp)[temp == max(temp)]
  }

x = c(1,2,3,3,3,4,4,5,5,5,6)
mode(x)
```

<br>

# Performing correlations and multivariate analysis

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/2.PNG"))
```

```{r fig.width=12, fig.height=7}
# Covariance matrix
cov(mtcars[1:3])

# Correlation matrix
cor(mtcars[1:3])

# Plotting heatmap
library(reshape2)
qplot(x=Var1, y=Var2, data=melt(cor(mtcars[1:3])), fill=value,
        geom="tile")

```


<br>

# Operating linear regression and multivariate analysis

```{r fig.width=12, fig.height=7}
# Create linear regression
lmfit = lm(mtcars$mpg ~ mtcars$cyl)
lmfit

# Checking summary of the model
summary(lmfit)

# Anova test of the model - analysis of the variance function
anova(lmfit)

# Plotting the model
lmfit = lm(mtcars$mpg ~ mtcars$cyl)
plot(mtcars$cyl, mtcars$mpg)
abline(lmfit)

```

<br>

# Conducting an statistical tests

## Binomial test

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/3.PNG"))
include_graphics(paste0(source_path,"/images/4.PNG"))
```

Imagine we are in a dice game, and a new player comes in with his own dice. If we expect dices to be fair, then, overall and after many plays, more or less all numbers should have appeared 1/6 of the total times. If the new player always plays to win with number 5, and we have computed he has won 92 times out of 315, we can determine if the dice he was using could be fake.

```{r fig.width=12, fig.height=7}
binom.test(x=92, n=315, p=1/6)
```

## Student t-test

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/5.PNG"))
```

### 1-sample t-test

```{r fig.width=12, fig.height=7}
## ONLY COMPARING MPG

# Visualise attributes against each other - it is clear that the mean of "automobile" is different to the "overall" mean
boxplot(mtcars$mpg, mtcars$mpg[mtcars$am==0], ylab = "mpg", names=c("overall","automobile"))
abline(h=mean(mtcars$mpg),lwd=2, col="red")
abline(h=mean(mtcars$mpg[mtcars$am==0]),lwd=2, col="blue")

# Check this with a T-test
mpg.mu = mean(mtcars$mpg)
mpg_am = mtcars$mpg[mtcars$am == 0]
t.test(mpg_am,mu = mpg.mu)
```

### 2-sample t-test

```{r fig.width=12, fig.height=7}
## COMPARING 2 VARIABLES

# Visualise attributes against each other - it is clear that the mean of "automobile" is different to the "manual" mean
boxplot(mtcars$mpg~mtcars$am,ylab='mpg',names=c('automatic','manual'))
abline(h=mean(mtcars$mpg[mtcars$am==0]),lwd=2, col="blue")
abline(h=mean(mtcars$mpg[mtcars$am==1]),lwd=2, col="red")

t.test(mtcars$mpg~mtcars$am)
```

<br>

## Kolmogorov-Smirnov Test

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/6.PNG"))
include_graphics(paste0(source_path,"/images/7.PNG"))
```

### 1-sample KS test

Comparing a sample with a reference probability. In the case below, we generate a random distribution X, and check what the KS test decides about it.

```{r fig.width=12, fig.height=7}
# Generate set
x = rnorm(50)

# KS test -> results show that the input is normally distributed as the p-value is bigger than 0.05 and we dont reject the null hypothesis.
ks.test(x,"pnorm")
```

### 2-sample KS test

Comparing cumulative distribution of 2 samples

```{r fig.width=12, fig.height=7}
# Generate sample data
set.seed(3)
x = runif(n=20, min=0, max=20)
y = runif(n=20, min=0, max=20)

# Checking visually
plot(ecdf(x), do.points = FALSE, verticals=T, xlim=c(0, 20))
lines(ecdf(y), lty=3, do.points = FALSE, verticals=T)

# KS test -> do both datasets come from the same distribution? Given the p-value is bigger than 0.05, we dont reject the null hypothesis and therefore, both datasets are possibly from the same distribution.
ks.test(x,y)
```

## Wilcoxon Rank Sum and Signed Rank Test

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/8.PNG"))
```

```{r fig.width=12, fig.height=7}
boxplot(mtcars$mpg~mtcars$am,ylab='mpg',names=c('automatic','manual'))

# Is the distribution of manual transmission cars the same as the distribution of automatic transmission cars? -> given that p-value is less than 0.05, we reject H0 and conclude that distributions are not the same.
wilcox.test(mpg ~ am, data=mtcars)
```

## Pearson chi-squared test

Determine whether the distribution of categorical variables of 2 groups differ.

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/9.PNG"))
include_graphics(paste0(source_path,"/images/10.PNG"))
include_graphics(paste0(source_path,"/images/11.PNG"))
include_graphics(paste0(source_path,"/images/12.PNG"))
```

```{r fig.width=12, fig.height=7}
ftable = table(mtcars$am, mtcars$gear)
ftable

mosaicplot(ftable, main="Number of Forward Gears Within
                        Automatic and Manual Cars", color = TRUE)

# Given that p-value is less than 0.05, we reject H0, therefore we conclude the distribution of both variables is not the same
chisq.test(ftable)

```

## ANOVA

```{r echo=FALSE, fig.width=5, fig.height=3}
include_graphics(paste0(source_path,"/images/13.PNG"))
include_graphics(paste0(source_path,"/images/14.PNG"))
```

### 1-way ANOVA

```{r fig.width=12, fig.height=7}
boxplot(mtcars$mpg~factor(mtcars$gear),xlab='gear',ylab='mpg')

# p-value less than 0.05 -> reject H0, therefore the mean of mpg changes with different types of gears
oneway.test(mtcars$mpg~factor(mtcars$gear))

# Same as above but with variance metrics
mtcars.aov = aov(mtcars$mpg ~ as.factor(mtcars$gear))
summary(mtcars.aov)

# Tukey multiple comparison of means
mtcars_posthoc =TukeyHSD(mtcars.aov)
mtcars_posthoc

# Plot shows that the differences between cars having 3 or 4 gears are the greatest because the confidence interval is furthest to the right of the plot. 
plot(mtcars_posthoc)
```

### 2-way ANOVA

This is the extension of a 1-way ANOVA since the analysis covers more than 2 categorical variables rather than 1.

```{r fig.width=12, fig.height=7}
# PLOTTING
par(mfrow=c(1,2))

# Clearly there seems to be differences between gear, transmissions and mpg
boxplot(mtcars$mpg~mtcars$gear,subset=(mtcars$am==0),xlab='gear',
        ylab = "mpg",main='automatic')

boxplot(mtcars$mpg~mtcars$gear,subset=(mtcars$am==1),xlab='gear',
        ylab = "mpg", main='manual')

# With this interaction plot, we can caracterize the relationship between variables. The resulting plot shows that the number of gears does have an effect on the mean of the mpg, but does not show a positive relationship 
interaction.plot(mtcars$gear, mtcars$am, mtcars$mpg, type="b",
                 col=c(1:3),leg.bty="o", leg.bg="beige", lwd=2, pch=c(18,24,22),
                 xlab="Number of Gears", ylab="Mean Miles Per Gallon",
                 main="Interaction Plot")

# 2-ways anova on MPG with combination of the other factors -> output shows that the p-value of the gear is lower than 0.05, therefore cars with different number of gears are more likely to have different means of mpg
mpg_anova2 = aov(mtcars$mpg~factor(mtcars$gear)*factor(mtcars$am))
summary(mpg_anova2)

TukeyHSD(mpg_anova2)

par(mfrow=c(1,2))
plot(TukeyHSD(mpg_anova2))
```







